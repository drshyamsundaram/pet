{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXRgwYKrFmvVy7sRGZoWBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drshyamsundaram/pet/blob/main/Dp_xgBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Differentially Private XGBoost Implementation\n",
        "\n",
        "This module provides a differentially private version of XGBoost for binary classification tasks.\n",
        "It implements the gradient-based approach to differential privacy in machine learning,\n",
        "as described in the following papers:\n",
        "\n",
        "[1] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016).\n",
        "    Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on\n",
        "    Computer and Communications Security (pp. 308-318).\n",
        "\n",
        "[2] Dwork, C., & Roth, A. (2014). The algorithmic foundations of differential privacy.\n",
        "    Foundations and Trends in Theoretical Computer Science, 9(3-4), 211-407.\n",
        "\n",
        "[3] Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System.\n",
        "    In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\n",
        "    and Data Mining (pp. 785-794).\n",
        "\n",
        "Author: [Shyam Sundaram]\n",
        "Date: [25 Nov 24 ]\n",
        "Version: 1.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import expit\n",
        "\n",
        "class DPXGBoost:\n",
        "    def __init__(self, epsilon, delta, num_trees, max_depth, learning_rate):\n",
        "        \"\"\"\n",
        "        Initialize the Differentially Private XGBoost model.\n",
        "\n",
        "        Args:\n",
        "            epsilon (float): Privacy parameter that controls the privacy budget.\n",
        "            delta (float): Probability of privacy violation.\n",
        "            num_trees (int): Number of trees in the ensemble.\n",
        "            max_depth (int): Maximum depth of each tree.\n",
        "            learning_rate (float): Step size shrinkage used to prevent overfitting.\n",
        "\n",
        "        The epsilon and delta parameters together define the privacy guarantee.\n",
        "        Smaller values provide stronger privacy but may reduce model accuracy.\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.num_trees = num_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "\n",
        "    def _add_noise(self, gradients, hessians):\n",
        "        \"\"\"\n",
        "        Add Laplace noise to gradients and hessians to achieve differential privacy.\n",
        "\n",
        "        Args:\n",
        "            gradients (np.array): Computed gradients.\n",
        "            hessians (np.array): Computed hessians.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Noisy gradients and hessians.\n",
        "\n",
        "        This method implements the Laplace mechanism as described in [2].\n",
        "        The noise scale is calculated based on the sensitivity of the gradients\n",
        "        and the privacy parameters (epsilon and delta).\n",
        "        \"\"\"\n",
        "        sensitivity = 2  # Assuming binary classification with log loss\n",
        "        noise_scale = sensitivity * np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon\n",
        "        noisy_gradients = gradients + np.random.laplace(0, noise_scale, size=gradients.shape)\n",
        "        noisy_hessians = hessians + np.random.laplace(0, noise_scale, size=hessians.shape)\n",
        "        return noisy_gradients, noisy_hessians\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Differentially Private XGBoost model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Training features.\n",
        "            y (np.array): Training labels.\n",
        "\n",
        "        This method implements the differentially private stochastic gradient descent\n",
        "        algorithm as described in [1], adapted for XGBoost. It iteratively builds\n",
        "        trees using noisy gradients and hessians.\n",
        "        \"\"\"\n",
        "        for _ in range(self.num_trees):\n",
        "            gradients = -y + expit(self.predict(X))\n",
        "            hessians = expit(self.predict(X)) * (1 - expit(self.predict(X)))\n",
        "\n",
        "            noisy_gradients, noisy_hessians = self._add_noise(gradients, hessians)\n",
        "\n",
        "            dtrain = xgb.DMatrix(X, label=y)\n",
        "            dtrain.set_base_margin(self.predict(X))\n",
        "\n",
        "            params = {\n",
        "                'objective': 'binary:logistic',\n",
        "                'max_depth': self.max_depth,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'silent': 1\n",
        "            }\n",
        "\n",
        "            model = xgb.train(params, dtrain, num_boost_round=1,\n",
        "                              obj=lambda _, pred: (noisy_gradients, noisy_hessians))\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained Differentially Private XGBoost model.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Features to predict.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Predicted probabilities.\n",
        "\n",
        "        This method aggregates predictions from all trees in the ensemble,\n",
        "        as described in [3], but with the differentially private trees trained\n",
        "        in the fit method.\n",
        "        \"\"\"\n",
        "        if not self.models:\n",
        "            return np.zeros(X.shape[0])\n",
        "\n",
        "        dtest = xgb.DMatrix(X)\n",
        "        predictions = sum(model.predict(dtest) for model in self.models)\n",
        "        return predictions\n",
        "\n",
        "'''\n",
        "# Comment if you want to execute without UI\n",
        "\n",
        "# Usage example\n",
        "X, y = np.random.rand(1000, 10), np.random.randint(0, 2, 1000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dp_xgb = DPXGBoost(epsilon=0.1, delta=1e-5, num_trees=10, max_depth=3, learning_rate=0.1)\n",
        "dp_xgb.fit(X_train, y_train)\n",
        "\n",
        "predictions = dp_xgb.predict(X_test)\n",
        "'''\n",
        "\n",
        "# UI for execution\n",
        "# prompt: Create a UI for the configuration for the code with model execute button\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import expit\n",
        "\n",
        "# ... (Your DPXGBoost class code from the previous response) ...\n",
        "\n",
        "# UI elements\n",
        "epsilon_slider = widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01, description='Epsilon:')\n",
        "delta_slider = widgets.FloatLogSlider(value=1e-5, min=-10, max=-1, step=0.1, description='Delta:')\n",
        "num_trees_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description='Num Trees:')\n",
        "max_depth_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Max Depth:')\n",
        "learning_rate_slider = widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Learning Rate:')\n",
        "execute_button = widgets.Button(description='Execute Model')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Layout\n",
        "input_widgets = widgets.VBox([epsilon_slider, delta_slider, num_trees_slider, max_depth_slider, learning_rate_slider, execute_button])\n",
        "display(widgets.HBox([input_widgets, output_area]))\n",
        "\n",
        "# Execute button click handler\n",
        "def on_button_clicked(b):\n",
        "    with output_area:\n",
        "        clear_output()  # Clear previous output\n",
        "        try:\n",
        "            # Get parameter values from the UI\n",
        "            epsilon = epsilon_slider.value\n",
        "            delta = delta_slider.value\n",
        "            num_trees = num_trees_slider.value\n",
        "            max_depth = max_depth_slider.value\n",
        "            learning_rate = learning_rate_slider.value\n",
        "\n",
        "            # Example data (replace with your actual data)\n",
        "            X, y = np.random.rand(1000, 10), np.random.randint(0, 2, 1000)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Create and train the model\n",
        "            dp_xgb = DPXGBoost(epsilon, delta, num_trees, max_depth, learning_rate)\n",
        "            dp_xgb.fit(X_train, y_train)\n",
        "            predictions = dp_xgb.predict(X_test)\n",
        "\n",
        "            print(\"Model execution complete.\")\n",
        "            print(\"Predictions:\", predictions) # Display some output\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "execute_button.on_click(on_button_clicked)"
      ],
      "metadata": {
        "id": "R2n_ZX93Xbd4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}